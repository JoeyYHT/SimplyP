{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "* Add f_IExcess back in for this time period and see if it still runs  \n",
    "* If it doesn't, decrease the maximum value to 0.1 and try again  \n",
    "* If it does, run for a longer period (as before when didn't work), but with the maximum value at 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, seaborn as sn, emcee, corner, mpld3\n",
    "from scipy import optimize\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from scipy.integrate import odeint\n",
    "from scipy.stats import norm\n",
    "\n",
    "sn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION USED BY SCIPY.INTEGRATE.ODEINT TO SOLVE THE MODEL EQUATIONS AT EACH TIME STEP\n",
    "\n",
    "def f(y, t, ode_params):\n",
    "    \"\"\" Define ODE system.\n",
    "            y is list [Vs, Qs, Qg, Ds, Dg]\n",
    "            t is an array of time points of interest\n",
    "            params is a tuple of input values & model params\n",
    "            (P, E, f_IExcess, alpha, beta, T_s, T_g, fc)\n",
    "    \"\"\"\n",
    "    # Unpack incremental values for Qs and Qg\n",
    "    Vs_i = y[0]\n",
    "    Qs_i = y[1]\n",
    "    Qg_i = y[2]\n",
    "    \n",
    "    # Unpack params\n",
    "    P, E, f_IExcess, alpha, beta, T_s, T_g, fc = ode_params\n",
    "\n",
    "    # Model equations (see section 2.2)\n",
    "    ##dQq_dt = (f_IExcess*P - Qq_i)/T_q\n",
    "    dQs_dV = (((Vs_i - fc)*np.exp(fc - Vs_i))/(T_s*((np.exp(fc-Vs_i) + 1)**2))) + (1/(T_s*(np.exp(fc-Vs_i) + 1)))\n",
    "    dVs_dt = P*(1-f_IExcess) - alpha*E*(1 - np.exp(-0.02*Vs_i)) - Qs_i\n",
    "    dQs_dt = dQs_dV*dVs_dt\n",
    "    dQg_dt = (beta*Qs_i - Qg_i)/T_g\n",
    "    dDs_dt = (1 - beta)*Qs_i\n",
    "    dDg_dt = Qg_i\n",
    "    \n",
    "    # Add results of equations to an array\n",
    "    res = np.array([dVs_dt, dQs_dt, dQg_dt, dDs_dt, dDg_dt])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO WRAP THE MODEL EQUATIONS IN A LOOP TO EVALUATE DRAINAGE VOLUME AT EACH STEP IN THE INPUT TIME SERIES\n",
    "\n",
    "def hydro_model(met_df, ics, p, period, step_len=1):\n",
    "    \"\"\" The hydrological model\n",
    "\n",
    "            met_df         Dataframe containing columns 'Rainfall_mm' and 'PET_mm', with datetime index\n",
    "            ics            Vector of initial conditions [Vs0, Vg0]\n",
    "            p              Series of parameter values (index = param name)\n",
    "            period         Vector of [start, end] dates [yyyy-mm-dd, yyyy-mm-dd]\n",
    "            step_len       Length of each step in the input dataset (days)\n",
    "\n",
    "        Returns a dataframe with column headings\n",
    "        [Vs, Qs, Qg, Ds, Dg, Sim_Runoff, Obs_Runoff]\n",
    "    \"\"\"\n",
    "    # Truncate the met data to the desired period\n",
    "    input_df = met_df.truncate(before=period[0], after=period[1])\n",
    "\n",
    "    # Unpack initial conditions\n",
    "    Vs0, Vg0 = ics\n",
    "\n",
    "    # Unpack model parameters\n",
    "    #     f_IExcess, alpha, beta, T_s, T_g, fc = mod_params\n",
    "\n",
    "    # Time points to evaluate ODEs at. We're only interested in the start and the end of each step\n",
    "    ti = [0, step_len]\n",
    "\n",
    "    # Lists to store output\n",
    "    output_ODEs = []\n",
    "    output_rest = []\n",
    "\n",
    "    # Loop over met data\n",
    "    for idx in range(len(input_df)):\n",
    "\n",
    "        # Get P and E for this day\n",
    "        P = input_df.ix[idx, 'Rainfall_mm']\n",
    "        E = input_df.ix[idx, 'PET_mm']\n",
    "\n",
    "        # Calculate infiltration excess and add to results\n",
    "        Qq = p['f_IExcess']*P\n",
    "        output_rest.append(Qq)\n",
    "\n",
    "        # Calculate Qs0 and Qg0 from Vs0 and Vg0\n",
    "        Qs0 = (Vs0 - p['fc'])/(p['T_s']*(1 + np.exp(p['fc'] - Vs0)))\n",
    "        Qg0 = Vg0/p['T_g']\n",
    "\n",
    "        # Vector of initial conditions\n",
    "        y0 = [Vs0, Qs0, Qg0, 0., 0.]\n",
    "\n",
    "        # Model parameters plus rainfall and ET, for input to solver\n",
    "        ode_params = np.array([P, E, p['f_IExcess'], p['alpha'], p['beta'], p['T_s'],\n",
    "                               p['T_g'], p['fc']])\n",
    "\n",
    "        # Solve\n",
    "        y = odeint(f, y0, ti, args=(ode_params,))\n",
    "\n",
    "        # Extract values for end of step\n",
    "        res = y[1]\n",
    "\n",
    "        # Numerical errors may result in very tiny values <0\n",
    "        # set these back to 0\n",
    "        res[res<0] = 0\n",
    "        output_ODEs.append(res)\n",
    "\n",
    "        # Update initial conditions for next step\n",
    "        Vs0 = res[0]\n",
    "        Vg0 = res[2]*p['T_g']\n",
    "\n",
    "    # Build a dataframe of ODE results\n",
    "    df1 = pd.DataFrame(data=np.vstack(output_ODEs),\n",
    "                      columns=['Vs', 'S', 'G', 'Ds', 'Dg'],\n",
    "                      index=input_df.index)\n",
    "\n",
    "    # Dataframe of non ODE results\n",
    "    df2 = pd.DataFrame(data=np.vstack(output_rest), columns=['Qq'],\n",
    "                     index=input_df.index)\n",
    "\n",
    "    # Concatenate results dataframes\n",
    "    df = pd.concat([df1,df2], axis=1)\n",
    "\n",
    "    # Estimate runoff as Ds + Dg\n",
    "    df['Sim_Runoff_mm_IE'] = df['Ds'] + df['Dg'] + df['Qq']\n",
    "    df['Sim_Runoff_mm'] = df['Ds'] + df['Dg']\n",
    "\n",
    "    # Add observed runoff to df\n",
    "    df['Obs_Runoff_mm'] = input_df['Runoff_mm']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO DEFINE LOG LIKELIHOOD\n",
    "\n",
    "def log_likelihood(p_cal_values, p_list, p, met_df, ics, period):\n",
    "    \"\"\" p_cal_values   Row vector of value for parameters being calibrated\n",
    "        p_list         List of parameters to be calibrated/varied\n",
    "        p              Series of all parameter values, including those to be calibrated (index = param names)\n",
    "        met_df         Dataframe including columns 'Rainfall_mm' and 'PET_mm', with datetime index\n",
    "        ics            List of initial conditions for Vs and Vg, e.g. [200., 100.]\n",
    "        \n",
    "        Returns the log likelihood.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Update parameter values being calibrated in the parameter Series, p\n",
    "    for idx, param in enumerate(p_list):\n",
    "        p[param] = p_cal_values[idx]\n",
    "\n",
    "    # Run deterministic model with these parameters\n",
    "    df = hydro_model(met_df=met_df, ics=ics, period=period, p=p)\n",
    "\n",
    "    # Extract arrays for simulated and observed runoff\n",
    "    sim = df['Sim_Runoff_mm']\n",
    "    obs = df['Obs_Runoff_mm']\n",
    "\n",
    "    # Calculate sigma_e for each step\n",
    "    sigma_e = p['m']*sim\n",
    "\n",
    "    # Calculate log likelihood. For each element in the arrays sim, sigma_e and obs,\n",
    "    # this code calculates the log prob of drawing 'obs' from a Gaussian centred on \n",
    "    # 'sim' with std. dev. 'sigma_e'\n",
    "    likes = norm(sim, sigma_e).logpdf(obs)\n",
    "\n",
    "    # If flow is zero, sigma_e is zero and scipy.norm returns NaN\n",
    "    # Set these to -inf instead\n",
    "    likes[np.isnan(likes)] = -np.inf\n",
    "\n",
    "    # Sum log likes\n",
    "    ll = np.sum(likes)\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO DEFINE LOG PRIOR\n",
    "\n",
    "def log_prior(p_cal_values, priors_df):\n",
    "    \"\"\" p_cal_values is an array of parameter values for calibrating params\n",
    "        priors_df is a dataframe of min and max values for priors for each calibrated param\n",
    "    \n",
    "        Returns the log prior probability of p\n",
    "    \"\"\"\n",
    "    # Add the current parameter values to the priors dataframe\n",
    "    priors_df['value'] = p_cal_values\n",
    "\n",
    "    # Determine whether the current parameter values lie within the priors\n",
    "    # (add a boolean column to the dataframe, value 0 if param not in range)\n",
    "    priors_df['valid'] = np.where((priors_df['value']>= priors_df['min']) &\n",
    "                                  (priors_df['value']<priors_df['max']), 1, 0)\n",
    "\n",
    "    # If all parameters are within allowed ranges, return a constant\n",
    "    # Otherwise, the parameter set is invalid. In which case, it has\n",
    "    # probability 0, i.e. log prob = -inf\n",
    "    if sum(priors_df['valid']) == len(priors_df['valid']):\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO DEFINE LOG POSTERIOR\n",
    "\n",
    "def log_posterior(p_cal_values, p_list, p, met_df, priors_df, ics, period):\n",
    "    \"\"\" mcmc_params    Vector of parameters (model params for calibration + error variance)\n",
    "        met_df         Dataframe containing columns 'Rainfall_mm' and 'PET_mm', with datetime index\n",
    "        \n",
    "        Returns the log posterior.\n",
    "        The log posterior distribution is (proportional to) the sum of the log prior and the log likelihood\n",
    "    \"\"\"   \n",
    "    # Get log prior prob\n",
    "    log_pri = log_prior(p_cal_values, priors_df)\n",
    "\n",
    "    # Evaluate log likelihood if necessary\n",
    "    if np.isfinite(log_pri):\n",
    "        log_like = log_likelihood(p_cal_values, p_list, p, met_df, ics, period)\n",
    "        \n",
    "        # Calculate log posterior\n",
    "        return log_pri + log_like\n",
    "    \n",
    "    else:\n",
    "        # Log prior is -inf, so log posterior is -inf too\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO DECIDE ON STARTION LOCATIONS FOR EACH OF THE MCMC WALKERS\n",
    "# To do this: (1) use an optimiser to estimate the maximum of the posterior\n",
    "# (2) add a small amount of random noise so each walker slights from a slightly different location\n",
    "\n",
    "def neg_log_posterior(p_cal_values, p_list, p, met_df, priors_df, ics, period):\n",
    "    \"\"\" Negative of log posterior.\n",
    "    \"\"\"\n",
    "    return -log_posterior(p_cal_values, p_list, p, met_df, priors_df, ics, period)\n",
    "\n",
    "def find_map(init_guess, p_list, p, met_df, priors_df, ics, period):\n",
    "    \"\"\" Estimate the location of the maximum of the posterior density.\n",
    "    \n",
    "            init_guess   Initial guess for starting optimiser\n",
    "            met_df       Data frame of meteorological data\n",
    "    \"\"\"\n",
    "    # Run optimiser\n",
    "    param_est = optimize.fmin(neg_log_posterior, init_guess, args=(p_list, p, met_df, priors_df, ics, period))\n",
    "\n",
    "    return param_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SET UP\n",
    "\n",
    "# USER INPUT\n",
    "\n",
    "# Input data. Download into a dataframe\n",
    "data_url = r'https://drive.google.com/uc?export=&id=0BximeC_RweaecHNIZF9GMHkwaWc'\n",
    "\n",
    "# Simulation period\n",
    "st_dt = '2004-04-01'  # Start date\n",
    "end_dt = '2004-08-31' # End date\n",
    "\n",
    "# Catchment area (m2)\n",
    "cat_area = 51.7E6\n",
    "\n",
    "# Model parameters, including starting guesses for those being calibrated.\n",
    "# Include parameters that define the error variance (e.g. sigma or m)\n",
    "param_dict = {'fc':200., 'beta':0.6, 'f_IExcess':0.02, 'alpha':0.75,\n",
    "              'T_s':10.,'T_g':100.,'m':0.5}\n",
    "# beta = 0.6            # BFI (dimensionless)\n",
    "# fc = 200.             # Field capacity (mm) #SSKIB: 290\n",
    "# f_IExcess = 0.02\n",
    "\n",
    "# Initial conditions\n",
    "Vs0_init = param_dict['fc']       # Initial soil volume (mm)\n",
    "Vg0_init = 90.       # Initial groundwater volume (mm)\n",
    "\n",
    "# list of params to calibrate\n",
    "p_list = ['f_IExcess', 'alpha','T_s','T_g','m']\n",
    "\n",
    "# Define priors for each of the params being calibrated. Just involves defining upper limits\n",
    "# (for now, assume uniform; lower limits all 0). Can change lower limits by changing input\n",
    "# to priors_df (below)\n",
    "max_dict = {'f_IExcess':1, 'alpha':2,'T_s':20,'T_g':500,'m':1}\n",
    "\n",
    "# ADMIN\n",
    "ics=[Vs0_init, Vg0_init]  # Initial conditions\n",
    "period=[st_dt, end_dt]    # Simulation period\n",
    "\n",
    "# # Dictionary of parameter values to be calibrated\n",
    "# p_cal = p.copy()\n",
    "# for param in p.keys():\n",
    "#     if param not in p_list:\n",
    "#         del p_cal[param]\n",
    "\n",
    "# Store parameter values in series, referenced by their names (the row indices)\n",
    "p = pd.Series(param_dict)  # All params; cols = param names, one row with values\n",
    "# p.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# Array of values for parameters to be calibrated. Note, this has the same ordering as \n",
    "# p_list\n",
    "p_cal = p[p_list] # Same as p, but only with parameters to be calibrated\n",
    "p_cal_values = p_cal.values\n",
    "\n",
    "# Create a dataframe of priors for parameters being calibrated (to be input to the 'prior' function)\n",
    "cols = ['min','max','value','valid']\n",
    "priors_df = pd.DataFrame(columns=cols, index=p_list)\n",
    "priors_df['min'] = np.zeros(len(p_list))\n",
    "priors_df['max'] = pd.Series(max_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# READ IN INPUT DATA\n",
    "met_df = pd.read_csv(data_url, parse_dates=True, dayfirst=True, index_col=0)\n",
    "\n",
    "# Convert cumecs to mm\n",
    "met_df['Runoff_mm'] = met_df['Q_Cumecs']*60*60*24*1000/cat_area\n",
    "del met_df['Q_Cumecs']\n",
    "\n",
    "# Linear interpolation of any missing values\n",
    "met_df.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -45.265279\n",
      "         Iterations: 342\n",
      "         Function evaluations: 574\n",
      "\n",
      "\n",
      "Estimated f_IExcess: 0.35\n",
      "Estimated alpha: 0.53\n",
      "Estimated T_s: 2.44\n",
      "Estimated T_g: 87.99\n",
      "Estimated m: 0.22\n"
     ]
    }
   ],
   "source": [
    "# RUN THE OPTIMISER TO FIND THE MAP\n",
    "\n",
    "LL = log_likelihood(p_cal_values, p_list, p, met_df, ics, period)\n",
    "L_prior = log_prior(p_cal, priors_df)\n",
    "L_post = log_posterior(p_cal_values, p_list, p, met_df, priors_df, ics, period)\n",
    "neg_L_post = neg_log_posterior(p_cal_values, p_list, p, met_df, priors_df, ics, period)\n",
    "\n",
    "# print LL\n",
    "# print L_prior\n",
    "# print L_post\n",
    "# print neg_L_post\n",
    "\n",
    "# Run optimiser\n",
    "param_est = find_map(p_cal_values, p_list, p, met_df, priors_df, ics, period)\n",
    "\n",
    "# Print results\n",
    "print '\\n'\n",
    "for idx, param in enumerate(p_cal.index):\n",
    "    print 'Estimated %s: %.2f' % (param, param_est[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
